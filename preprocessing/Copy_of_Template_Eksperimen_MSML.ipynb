{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZLRMFl0JyyQ"
      },
      "source": [
        "# **1. Perkenalan Dataset**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hssSDn-5n3HR"
      },
      "source": [
        "Tahap pertama, Anda harus mencari dan menggunakan dataset dengan ketentuan sebagai berikut:\n",
        "\n",
        "1. **Sumber Dataset**:  \n",
        "   Dataset dapat diperoleh dari berbagai sumber, seperti public repositories (*Kaggle*, *UCI ML Repository*, *Open Data*) atau data primer yang Anda kumpulkan sendiri.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKADPWcFKlj3"
      },
      "source": [
        "# **2. Import Library**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgA3ERnVn84N"
      },
      "source": [
        "Pada tahap ini, Anda perlu mengimpor beberapa pustaka (library) Python yang dibutuhkan untuk analisis data dan pembangunan model machine learning atau deep learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlmvjLY9M4Yj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, OrdinalEncoder, PowerTransformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from imblearn.over_sampling import SMOTE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3YIEnAFKrKL"
      },
      "source": [
        "# **3. Memuat Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey3ItwTen_7E"
      },
      "source": [
        "Pada tahap ini, Anda perlu memuat dataset ke dalam notebook. Jika dataset dalam format CSV, Anda bisa menggunakan pustaka pandas untuk membacanya. Pastikan untuk mengecek beberapa baris awal dataset untuk memahami strukturnya dan memastikan data telah dimuat dengan benar.\n",
        "\n",
        "Jika dataset berada di Google Drive, pastikan Anda menghubungkan Google Drive ke Colab terlebih dahulu. Setelah dataset berhasil dimuat, langkah berikutnya adalah memeriksa kesesuaian data dan siap untuk dianalisis lebih lanjut.\n",
        "\n",
        "Jika dataset berupa unstructured data, silakan sesuaikan dengan format seperti kelas Machine Learning Pengembangan atau Machine Learning Terapan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHCGNTyrM5fS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Cara Sederhana: Menggunakan relative path\n",
        "# Tanda '..' artinya naik satu level ke folder di atasnya\n",
        "try:\n",
        "    df = pd.read_csv('../bank-additional-full_raw.csv', sep=';')\n",
        "except FileNotFoundError:\n",
        "    # Opsi cadangan jika dijalankan dari folder root (misal di VS Code tertentu)\n",
        "    df = pd.read_csv('Eksperimen_SML_Moch-Arief-Kresnanda/bank-additional-full_raw.csv', sep=';')\n",
        "\n",
        "print(\"Dataset berhasil dimuat:\")\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgZkbJLpK9UR"
      },
      "source": [
        "# **4. Exploratory Data Analysis (EDA)**\n",
        "\n",
        "Pada tahap ini, Anda akan melakukan **Exploratory Data Analysis (EDA)** untuk memahami karakteristik dataset.\n",
        "\n",
        "Tujuan dari EDA adalah untuk memperoleh wawasan awal yang mendalam mengenai data dan menentukan langkah selanjutnya dalam analisis atau pemodelan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKeejtvxM6X1"
      },
      "outputs": [],
      "source": [
        "# dataset info\n",
        "print(\"Dataset Info:\")\n",
        "df.info()\n",
        "\n",
        "# missing values\n",
        "print(\"\\nMissing Values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# duplicates\n",
        "print(f\"\\nDuplicate Rows: {df.duplicated().sum()}\")\n",
        "\n",
        "# Statistical summary\n",
        "display(df.describe())\n",
        "\n",
        "# Target variable distribution\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x='y', data=df)\n",
        "plt.title('Distribution of Target Variable (y)')\n",
        "plt.show()\n",
        "\n",
        "# Check categorical variables\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "for col in categorical_cols:\n",
        "    print(f\"\\nUnique values in {col}:\")\n",
        "    print(df[col].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpgHfgnSK3ip"
      },
      "source": [
        "# **5. Data Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COf8KUPXLg5r"
      },
      "source": [
        "Pada tahap ini, data preprocessing adalah langkah penting untuk memastikan kualitas data sebelum digunakan dalam model machine learning.\n",
        "\n",
        "Jika Anda menggunakan data teks, data mentah sering kali mengandung nilai kosong, duplikasi, atau rentang nilai yang tidak konsisten, yang dapat memengaruhi kinerja model. Oleh karena itu, proses ini bertujuan untuk membersihkan dan mempersiapkan data agar analisis berjalan optimal.\n",
        "\n",
        "Berikut adalah tahapan-tahapan yang bisa dilakukan, tetapi **tidak terbatas** pada:\n",
        "1. Menghapus atau Menangani Data Kosong (Missing Values)\n",
        "2. Menghapus Data Duplikat\n",
        "3. Normalisasi atau Standarisasi Fitur\n",
        "4. Deteksi dan Penanganan Outlier\n",
        "5. Encoding Data Kategorikal\n",
        "6. Binning (Pengelompokan Data)\n",
        "\n",
        "Cukup sesuaikan dengan karakteristik data yang kamu gunakan yah. Khususnya ketika kami menggunakan data tidak terstruktur."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Og8pGV0-iDLz"
      },
      "outputs": [],
      "source": [
        "# 1. Drop 'duration' column\n",
        "df_clean = df.drop(columns=['duration'])\n",
        "\n",
        "# 2. Handle duplicates\n",
        "df_clean = df_clean.drop_duplicates()\n",
        "print(f\"Shape after dropping duplicates: {df_clean.shape}\")\n",
        "\n",
        "# --- TAMBAHAN 1: Feature Engineering pdays ---\n",
        "# Ubah pdays 999 menjadi kategori baru atau biner\n",
        "df_clean['previously_contacted'] = np.where(df_clean['pdays'] == 999, 0, 1)\n",
        "df_clean = df_clean.drop(columns=['pdays']) \n",
        "\n",
        "# --- TAMBAHAN 2: Capping Outliers pada 'campaign' ---\n",
        "# Membatasi nilai campaign di persentil 99 agar tidak ada nilai ekstrem\n",
        "upper_limit = df_clean['campaign'].quantile(0.99)\n",
        "df_clean['campaign'] = np.where(df_clean['campaign'] > upper_limit, upper_limit, df_clean['campaign'])\n",
        "\n",
        "# 3. Separate features and target\n",
        "X = df_clean.drop(columns=['y'])\n",
        "y = df_clean['y']\n",
        "\n",
        "# 4. Encode target variable\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "print(\"Target classes:\", le.classes_)\n",
        "\n",
        "# 5. Identify columns types\n",
        "# --- REVISI STRATEGI ENCODING (HYBRID) ---\n",
        "\n",
        "# A. Ordinal (Punya urutan atau Biner)\n",
        "ordinal_cols = ['education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week']\n",
        "\n",
        "# Definisikan urutan untuk masing-masing\n",
        "education_order = ['illiterate', 'basic.4y', 'basic.6y', 'basic.9y', 'high.school', 'professional.course', 'university.degree', 'unknown']\n",
        "default_order = ['no', 'yes', 'unknown']\n",
        "housing_order = ['no', 'yes', 'unknown']\n",
        "loan_order    = ['no', 'yes', 'unknown']\n",
        "contact_order = ['telephone', 'cellular']\n",
        "# REVISI: Tambahkan jan dan feb agar urutan lengkap secara logis\n",
        "month_order   = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n",
        "day_order     = ['mon', 'tue', 'wed', 'thu', 'fri']\n",
        "\n",
        "all_ordinal_categories = [\n",
        "    education_order, default_order, housing_order, loan_order, \n",
        "    contact_order, month_order, day_order\n",
        "]\n",
        "\n",
        "# B. Nominal (Murni Kategori, tanpa urutan) -> Tetap OneHot\n",
        "categorical_cols = ['job', 'marital', 'poutcome']\n",
        "\n",
        "# C. Numerical\n",
        "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "# Pastikan kolom yang sudah masuk ordinal/categorical tidak masuk numerical\n",
        "numerical_cols = [col for col in numerical_cols if col not in ordinal_cols and col not in categorical_cols]\n",
        "\n",
        "print(f\"Numerical columns: {list(numerical_cols)}\")\n",
        "print(f\"Nominal (OneHot) columns: {list(categorical_cols)}\")\n",
        "print(f\"Ordinal columns: {list(ordinal_cols)}\")\n",
        "\n",
        "# 6. Preprocessing pipeline\n",
        "# Numerical\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('yeo_johnson', PowerTransformer(method='yeo-johnson')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Categorical (Nominal): OneHot\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')), \n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "])\n",
        "\n",
        "# Categorical (Ordinal): Ordinal Encoding\n",
        "ordinal_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')), \n",
        "    ('ordinal', OrdinalEncoder(categories=all_ordinal_categories, handle_unknown='use_encoded_value', unknown_value=-1))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat_nominal', categorical_transformer, categorical_cols),\n",
        "        ('cat_ordinal', ordinal_transformer, ordinal_cols)\n",
        "    ])\n",
        "\n",
        "# 7. Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# 8. Apply preprocessing\n",
        "# Fit on training data, transform on both\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "# --- TAMBAHAN 3: Handle Imbalance dengan SMOTE ---\n",
        "print(\"Before SMOTE counts:\", np.bincount(y_train))\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_processed, y_train)\n",
        "print(\"After SMOTE counts:\", np.bincount(y_train_resampled))\n",
        "\n",
        "# Update variabel training data ke hasil resampling\n",
        "X_train_processed = X_train_resampled\n",
        "y_train = y_train_resampled\n",
        "\n",
        "# Get feature names\n",
        "try:\n",
        "    cat_nominal_names = preprocessor.named_transformers_['cat_nominal']['onehot'].get_feature_names_out(categorical_cols)\n",
        "    feature_names = list(numerical_cols) + list(cat_nominal_names) + list(ordinal_cols)\n",
        "    \n",
        "    # Convert back to dataframe for verification\n",
        "    X_train_df = pd.DataFrame(X_train_processed, columns=feature_names)\n",
        "    print(\"Processed Train Data Shape:\", X_train_df.shape)\n",
        "    display(X_train_df.head())\n",
        "except Exception as e:\n",
        "    print(f\"Could not create DataFrame: {e}\")\n",
        "    print(\"Processed Train Data Shape:\", X_train_processed.shape)\n",
        "\n",
        "print(\"Preprocessing complete.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "credit_scoring",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
